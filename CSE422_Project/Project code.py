# -*- coding: utf-8 -*-
"""Group06_23101445_22299088

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a4UWEw61nSMbHxecQx66-cgxlxLM6Cqy

# **Import** **Packages**
"""

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_transformer
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from itertools import cycle
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

"""# **Data** **Import**"""

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv("/content/drive/MyDrive/CSE422 project/Customer_Category_Classifier_Dataset.csv")

"""# **Data Visualization**"""

print (f'Shape of the dataset is {data.shape}. This dataset contains {data.shape[0]} rows and {data.shape[1]} columns.')

data.head(5)

data.info()

numerical_data = data.select_dtypes(include='number')
numerical_features=numerical_data.columns.tolist()
print(f'There are {len(numerical_features)} numerical features:', '\n')
print(numerical_features)

categorical_data=data.select_dtypes(include= 'object')
categorical_features=categorical_data.columns.tolist()
print(f'There are {len(categorical_features)} catagorical features:', '\n')
print(categorical_features)

numerical_data.nunique()

numerical_data.isnull().sum()

categorical_data.isnull().sum()

categorical_data.nunique()

for col in categorical_features:
    ax = categorical_data[col].value_counts().sort_index().plot(kind='bar', figsize=(10,6))
    ax.set_title(f'Distribution of {col}')
    ax.set_xlabel(col)
    ax.set_ylabel('Count')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

numerical_data.hist(figsize=(20,15),bins=20)
plt.show()

"""# **Heatmap and Correlation**"""

data['Segmentation'].unique()

cor_data=data.copy()
cor_data=cor_data.drop(['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Var_1', 'Spending_Score'], axis=1)
cor_data['Segmentation'] = cor_data['Segmentation'].map({'A': 0, 'B': 1, 'C': 2, 'D':3})

correlation_matrix = cor_data.corr()
correlation_matrix

plt.figure(figsize=(10,5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=1)
plt.title("Correlation Heatmap", fontsize=16)
plt.show()

"""# **Pre-Processing**"""

print("Shape of dataframe before droping", data.shape) # null value delete of categorical data
data=data.dropna(axis=0,subset=['Ever_Married','Graduated','Profession','Var_1'])
print("Shape of dataframe after droping", data.shape)

imputer = SimpleImputer(missing_values=np.nan, strategy='mean') #in numerical value replace missing/null value with mean impute
data[['Work_Experience', 'Family_Size']] = imputer.fit_transform(data[['Work_Experience', 'Family_Size']])
data.shape

data['Segmentation'] = data['Segmentation'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3})

categorical_features =['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Var_1', 'Spending_Score']
data = data.drop(['ID'], axis=1)
le = LabelEncoder()
for col in categorical_features:
    data[col] = le.fit_transform(data[col])
data.head(5)

X = data.drop("Segmentation", axis=1)
y = data["Segmentation"]                                        #dataset split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

scaler = StandardScaler() #feature scaling
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Model **training**"""

models = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42), #sigmoid function
    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
}

results = {}
class_names = [str(c) for c in sorted(y.unique())]

for name, model in models.items():

    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)
    y_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None

    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)
    results[name] = {
        'accuracy': accuracy,
        'classification_report': report,
        'confusion_matrix': confusion_matrix(y_test, y_pred),
        'y_proba': y_proba
    }

    print(f"\nResults for {name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=class_names))

    plt.figure(figsize=(6, 4))
    sns.heatmap(results[name]['confusion_matrix'], annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

accuracies = [results[name]['accuracy'] for name in results]
plt.figure(figsize=(8, 6))
sns.barplot(x=accuracies, y=list(results.keys()))
plt.title('Model Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Model')
plt.show()

y_test_bin = label_binarize(y_test, classes=[0, 1, 2,3])
plt.figure(figsize=(8, 6))
colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for (name, color) in zip(results, colors):
    if results[name]['y_proba'] is not None:
        y_score = results[name]['y_proba']
        fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, label=f'{name} (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (One-vs-Rest)')
plt.legend()
plt.grid(True)
plt.show()

#kmeans_unsupervised

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X_pca)
labels = kmeans.labels_

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', label='Centroids')
plt.title("KMeans Clustering (PCA-reduced Data)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend()
plt.show()